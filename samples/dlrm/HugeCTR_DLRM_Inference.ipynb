{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb9e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f771572a",
   "metadata": {},
   "source": [
    "# 1 Overview\n",
    "In this notebook, we want to provide a tutorial on how to use standard DLRM model that trained on HugeCTR_DLRM_Training.\n",
    "notebook and deploy the saved model to Triton Inference Server. We could collect the inference benchmark by Triton performance analyzer  tool\n",
    "\n",
    "1. [Overview](#1)\n",
    "2. [Generate the DLRM Deployment Configuration](#2)\n",
    "3. [Load Models on Triton Server](#3)\n",
    "4. [Prepare Inference Input Data](#4) \n",
    "5. [Inference Benchmarm by Triton Performance Tool](#5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7683ee9e",
   "metadata": {},
   "source": [
    "# 2. Generate the DLRM Deployment Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41fe864",
   "metadata": {},
   "source": [
    "## 2.1 Generate related model folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17fecafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some data folder to store the model related files\n",
    "# Standard Libraries\n",
    "import os\n",
    "from time import time\n",
    "import re\n",
    "import shutil\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "BASE_DIR = \"/dlrm_infer\"\n",
    "model_folder  = os.path.join(BASE_DIR, \"model\")\n",
    "dlrm_model_repo= os.path.join(model_folder, \"dlrm\")\n",
    "dlrm_version =os.path.join(dlrm_model_repo, \"1\")\n",
    "\n",
    "if os.path.isdir(model_folder):\n",
    "    shutil.rmtree(model_folder)\n",
    "os.makedirs(model_folder)\n",
    "\n",
    "if os.path.isdir(dlrm_model_repo):\n",
    "    shutil.rmtree(dlrm_model_repo)\n",
    "os.makedirs(dlrm_model_repo)\n",
    "\n",
    "if os.path.isdir(dlrm_version):\n",
    "    shutil.rmtree(dlrm_version)\n",
    "os.makedirs(dlrm_version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef9aa5",
   "metadata": {},
   "source": [
    "### 2.2 Copy DLRM model files to model repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a38764b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8193\r\n",
      "-rw-r--r-- 1 root root    3706 Nov 29 07:25 dlrm.json\r\n",
      "drwxr-xr-x 2 root root    4096 Nov 29 07:25 dlrm0_sparse_20000.model\r\n",
      "-rw-r--r-- 1 root root 9479684 Nov 29 07:25 dlrm_dense_20000.model\r\n"
     ]
    }
   ],
   "source": [
    "! cp -r /dlrm_train/dlrm0_sparse_20000.model $dlrm_version/\n",
    "! cp /dlrm_train/dlrm_dense_20000.model $dlrm_version/\n",
    "! cp /dlrm_train/dlrm.json $dlrm_version/\n",
    "!ls -l $dlrm_version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db35a79",
   "metadata": {},
   "source": [
    "### 2.3 Generate the Triton configuration for deploying DLRM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b0b5c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /dlrm_infer/model/dlrm/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile $dlrm_model_repo/config.pbtxt\n",
    "name: \"dlrm\"\n",
    "backend: \"hugectr\"\n",
    "max_batch_size:64,\n",
    "input [\n",
    "   {\n",
    "    name: \"DES\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"CATCOLUMN\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"ROWINDEX\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind : KIND_GPU\n",
    "    gpus:[0]\n",
    "  }\n",
    "]\n",
    "\n",
    "parameters [\n",
    "  {\n",
    "    key: \"config\"\n",
    "    value: { string_value: \"/dlrm_infer/model/dlrm/1/dlrm.json\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"gpucache\"\n",
    "    value: { string_value: \"true\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"hit_rate_threshold\"\n",
    "    value: { string_value: \"0.8\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"gpucacheper\"\n",
    "    value: { string_value: \"0.5\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"label_dim\"\n",
    "    value: { string_value: \"1\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"slots\"\n",
    "    value: { string_value: \"26\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"cat_feature_num\"\n",
    "    value: { string_value: \"26\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"des_feature_num\"\n",
    "    value: { string_value: \"13\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"max_nnz\"\n",
    "    value: { string_value: \"2\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"embedding_vector_size\"\n",
    "    value: { string_value: \"128\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"embeddingkey_long_type\"\n",
    "    value: { string_value: \"true\" }\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b98e5b",
   "metadata": {},
   "source": [
    "### 2.4 Generate the Hugectr Backend parameter server configuration for deploying dlrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "081dbb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /dlrm_infer/model/ps.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile $model_folder/ps.json\n",
    "{\n",
    "    \"supportlonglong\":true,\n",
    "    \"db_type\": \"local\",\n",
    "    \"models\":[\n",
    "        {\n",
    "            \"model\":\"dlrm\",\n",
    "            \"sparse_files\":[\"/dlrm_infer/model/dlrm/1/dlrm0_sparse_20000.model\"],\n",
    "            \"dense_file\":\"/dlrm_infer/model/dlrm/1/dlrm_dense_20000.model\",\n",
    "            \"network_file\":\"/dlrm_infer/model/dlrm/1/dlrm.json\",\n",
    "            \"num_of_worker_buffer_in_pool\": 4,\n",
    "            \"num_of_refresher_buffer_in_pool\":1,\n",
    "            \"deployed_device_list\":[0],\n",
    "            \"max_batch_size\":64,\n",
    "            \"default_value_for_each_table\":[0.0,0.0],\n",
    "            \"hit_rate_threshold\":0.9,\n",
    "            \"gpucacheper\":0.5,\n",
    "            \"gpucache\":true,\n",
    "            \"cache_refresh_percentage_per_iteration\":0.2,\n",
    "            \"maxnum_des_feature_per_sample\": 13,\n",
    "            \"maxnum_catfeature_query_per_table_per_sample\":[26],\n",
    "            \"embedding_vecsize_per_table\":[128],\n",
    "            \"slot_num\":26\n",
    "        }\n",
    "    ]  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d31c9b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 9281\n",
      "-rw-r--r-- 1 root root    3706 Nov 29 07:25 dlrm.json\n",
      "drwxr-xr-x 2 root root    4096 Nov 29 07:25 dlrm0_sparse_20000.model\n",
      "-rw-r--r-- 1 root root 9479684 Nov 29 07:25 dlrm_dense_20000.model\n",
      "total 1\n",
      "drwxr-xr-x 3 root root 4096 Nov 29 07:25 1\n",
      "-rw-r--r-- 1 root root 1177 Nov 29 07:25 config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!ls -l $dlrm_version\n",
    "!ls -l $dlrm_model_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f93bd5",
   "metadata": {},
   "source": [
    "## 3. Deploy DLRM on Triton Server\n",
    "At this stage, you should have already launched the Triton Inference Server with the following command:\n",
    "\n",
    "In this tutorial, we will deploy the DLRM to a single V100(32GB)\n",
    "\n",
    "docker run --gpus=all -it -v /dlrm_infer/:/dlrm_infer -v /dlrm_train/:/dlrm_train --net=host nvcr.io/nvidia/merlin/merlin-inference:22.06 /bin/bash\n",
    "\n",
    "After you enter into the container you can launch triton server with the command below:\n",
    "\n",
    "tritonserver --model-repository=/dlrm_infer/model/ --load-model=dlrm \n",
    "    --model-control-mode=explicit \n",
    "    --backend-directory=/usr/local/hugectr/backends \n",
    "    --backend-config=hugectr,ps=/dlrm_infer/model/ps.json \n",
    "    \n",
    "Note: The model-repository path is /dlrm_infer/model/. The path for the dlrm model network json file is /dlrm_infer/model/dlrm/1/dlrm.json. The path for the parameter server configuration file is /dlrm_infer/model/ps.json."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc666eac",
   "metadata": {},
   "source": [
    "## 4. Prepare Inference Input Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2966669",
   "metadata": {},
   "source": [
    "### 4.1 Read validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eaff2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 702242\r\n",
      "-rw-r--r-- 1 root root        33 Nov 29 06:42 _file_list.txt\r\n",
      "-rw-r--r-- 1 root root  81092112 Nov 29 06:42 _hugectr.keyset\r\n",
      "-rw-r--r-- 1 root root     21528 Nov 29 06:42 _metadata\r\n",
      "-rw-r--r-- 1 root root      1437 Nov 29 06:42 _metadata.json\r\n",
      "-rw-r--r-- 1 root root 128131055 Nov 29 06:42 part_0.parquet\r\n",
      "-rw-r--r-- 1 root root     19945 Nov 29 06:42 schema.pbtxt\r\n",
      "drwxr-xr-x 2 root root      4096 Nov 29 06:41 temp-parquet-after-conversion\r\n",
      "-rw-r--r-- 1 root root 509766965 Nov 29 06:39 test.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /dlrm_train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ea07a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_parquet('/dlrm_train/val/part_0.parquet',engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2304ab7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>...</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>I11</th>\n",
       "      <th>I12</th>\n",
       "      <th>I13</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>669260</td>\n",
       "      <td>28</td>\n",
       "      <td>473</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>157</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>19900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.209261</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.281810</td>\n",
       "      <td>0.035263</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>-0.261958</td>\n",
       "      <td>-0.173750</td>\n",
       "      <td>-0.262248</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.209261</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.258333</td>\n",
       "      <td>-0.760031</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>-0.261958</td>\n",
       "      <td>-0.194540</td>\n",
       "      <td>-0.262248</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>148</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240178</td>\n",
       "      <td>0.205918</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.276593</td>\n",
       "      <td>2.204247</td>\n",
       "      <td>1.386036</td>\n",
       "      <td>0.690729</td>\n",
       "      <td>-0.277389</td>\n",
       "      <td>0.046789</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>177</td>\n",
       "      <td>61</td>\n",
       "      <td>377</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>402</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023207</td>\n",
       "      <td>0.068484</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.276593</td>\n",
       "      <td>1.987348</td>\n",
       "      <td>1.386036</td>\n",
       "      <td>0.055604</td>\n",
       "      <td>-0.289136</td>\n",
       "      <td>-0.306396</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.526501</td>\n",
       "      <td>-0.206385</td>\n",
       "      <td>0.339399</td>\n",
       "      <td>-0.281810</td>\n",
       "      <td>0.396760</td>\n",
       "      <td>-0.470383</td>\n",
       "      <td>2.596102</td>\n",
       "      <td>-0.257140</td>\n",
       "      <td>0.399973</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       C1  C2   C3  C4  C5  C6   C7  C8  C9    C10  ...        I5        I6  \\\n",
       "0  669260  28  473  18   2   1  157   4   3  19900  ... -0.209261 -0.206385   \n",
       "1       1   1   31   2  20   1   51   3   1      1  ... -0.209261 -0.206385   \n",
       "2       2  25  148  52   2   1  101   6   5      2  ...  0.240178  0.205918   \n",
       "3     177  61  377   2   1   1  402  17   1    209  ...  0.023207  0.068484   \n",
       "4       2   3   30   4   2   1    8   1   2      2  ...  1.526501 -0.206385   \n",
       "\n",
       "         I7        I8        I9       I10       I11       I12       I13  label  \n",
       "0 -0.064249 -0.281810  0.035263 -0.470383 -0.261958 -0.173750 -0.262248    0.0  \n",
       "1 -0.064249 -0.258333 -0.760031 -0.470383 -0.261958 -0.194540 -0.262248    0.0  \n",
       "2 -0.064249 -0.276593  2.204247  1.386036  0.690729 -0.277389  0.046789    0.0  \n",
       "3 -0.064249 -0.276593  1.987348  1.386036  0.055604 -0.289136 -0.306396    0.0  \n",
       "4  0.339399 -0.281810  0.396760 -0.470383  2.596102 -0.257140  0.399973    0.0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cf5f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(200000).to_csv('infer_test.csv', sep=',', index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d8b4ee",
   "metadata": {},
   "source": [
    "### 4.2 Follow the Triton requirements to generate input data with json format for performance test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3b4d99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./criteo2predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./criteo2predict.py\n",
    "import argparse\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "def parse_config(src_config):\n",
    "    try:\n",
    "        with open(src_config, 'r') as data_json:\n",
    "            j_data = json.load(data_json)\n",
    "            dense_dim = j_data[\"dense\"]\n",
    "            categorical_dim = j_data[\"categorical\"]\n",
    "            slot_size = j_data[\"slot_size\"]\n",
    "        assert(categorical_dim == np.sum(slot_size))\n",
    "        return dense_dim, categorical_dim, slot_size\n",
    "    except:\n",
    "        print(\"Invalid data configuration file!\")\n",
    "\n",
    "def convert(src_csv, src_config, dst, batch_size,segmentation):\n",
    "    dense_dim, categorical_dim, slot_size = parse_config(src_config)\n",
    "    slot_size_array=[4976199, 25419, 14705, 7112, 19283, 4, 6391, 1282, 60, 3289052, 282487, 138210, 11, 2203, 8901, 67, 4, 948, 15, 5577159, 1385790, 4348882, 178673, 10023, 88, 34]\n",
    "    offset = np.insert(np.cumsum(slot_size_array), 0, 0)[:-1]\n",
    "    total_columns = 1 + dense_dim + categorical_dim\n",
    "    df = pd.read_csv(src_csv,  sep=',', nrows=batch_size)\n",
    "    cols = df.columns\n",
    "    slot_num = len(slot_size)\n",
    "    row_ptrs = [0 for _ in range(batch_size*slot_num + 1)]\n",
    "    for i in range(1, len(row_ptrs)):\n",
    "        row_ptrs[i] = row_ptrs[i-1] + slot_size[(i-1)%slot_num]\n",
    "    label_df =  pd.DataFrame(df['label'].values.reshape(1,batch_size))\n",
    "    dense_df = pd.DataFrame(df[['I'+str(i+1) for i in range(dense_dim)]].values.reshape(1, batch_size*dense_dim))\n",
    "    embedding_columns_df = pd.DataFrame(df[['C'+str(i+1) for i in range(categorical_dim)]].values.reshape(1, batch_size*categorical_dim))\n",
    "    row_ptrs_df = pd.DataFrame(np.array(row_ptrs).reshape(1, batch_size*slot_num + 1))\n",
    "    with open(dst, 'w') as dst_txt:\n",
    "        dst_txt.write(\"{\\n\\\"data\\\":[\\n{\\n\")\n",
    "        dst_txt.write(\"\\\"DES\\\":\")\n",
    "        dst_txt.write(','.join('%s' %id for id in dense_df.values.tolist()))\n",
    "        dst_txt.write(\",\\n\\\"CATCOLUMN\\\":\")\n",
    "        dst_txt.write(','.join('%s' %id for id in (embedding_columns_df.values.reshape(-1,26)+offset).reshape(1,-1).tolist()))\n",
    "        dst_txt.write(\",\\n\\\"ROWINDEX\\\":\")\n",
    "        dst_txt.write(','.join('%s' %id for id in row_ptrs_df.values.tolist()))\n",
    "        dst_txt.write(\"\\n}\\n]\\n}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    arg_parser = argparse.ArgumentParser(description='Convert Preprocessed Criteo Data to Inference Format')\n",
    "    arg_parser.add_argument('--src_csv_path', type=str, required=True)\n",
    "    arg_parser.add_argument('--src_config_path', type=str, required=True)\n",
    "    arg_parser.add_argument('--dst_path', type=str, required=True)\n",
    "    arg_parser.add_argument('--batch_size', type=int, default=128)\n",
    "    arg_parser.add_argument('--segmentation', type=str, default=' ')\n",
    "    args = arg_parser.parse_args()\n",
    "    src_csv_path = args.src_csv_path\n",
    "    segmentation = args.segmentation\n",
    "    src_config_path = args.src_config_path\n",
    "    dst_path = args.dst_path\n",
    "    batch_size = args.batch_size\n",
    "    convert(src_csv_path, src_config_path, dst_path, batch_size, segmentation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b759715",
   "metadata": {},
   "source": [
    "### 4.3 Define Inference Input Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb1f800c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./dlrm_input_format.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./dlrm_input_format.json\n",
    "{\n",
    "    \"dense\": 13,\n",
    "    \"categorical\": 26,\n",
    "    \"slot_size\": [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218fbc4d",
   "metadata": {},
   "source": [
    "### 4.4 Generate the input json data with batch size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a5b3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=1\n",
    "!python3 criteo2predict.py --src_csv_path=./infer_test.csv --src_config_path=dlrm_input_format.json --dst_path ./$batchsize\".json\" --batch_size=$batchsize --segmentation=','"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319a0f7c",
   "metadata": {},
   "source": [
    "### 4.4 Get Triton server status if deploy DLRM successfully in Step3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f472f56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 127.0.0.1:8000...\r\n",
      "* TCP_NODELAY set\r\n",
      "* Connected to localhost (127.0.0.1) port 8000 (#0)\r\n",
      "> GET /v2/health/ready HTTP/1.1\r\n",
      "\r\n",
      "> Host: localhost:8000\r\n",
      "\r\n",
      "> User-Agent: curl/7.68.0\r\n",
      "\r\n",
      "> Accept: */*\r\n",
      "\r\n",
      "> \r\n",
      "\r\n",
      "* Mark bundle as not supporting multiuse\r\n",
      "< HTTP/1.1 200 OK\r\n",
      "\r\n",
      "< Content-Length: 0\r\n",
      "\r\n",
      "< Content-Type: text/plain\r\n",
      "\r\n",
      "< \r\n",
      "\r\n",
      "* Connection #0 to host localhost left intact\r\n"
     ]
    }
   ],
   "source": [
    "!curl -v localhost:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072ebf5e",
   "metadata": {},
   "source": [
    "## 5. Get Inference benchmark by Triton Performance Tool "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930785f3",
   "metadata": {},
   "source": [
    "### 5.1 Get the inference performance for batchsize=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12858621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Successfully read data for 1 stream/streams with 1 step/steps.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Client: \n",
      "    Request count: 7323\n",
      "    Throughput: 1464.6 infer/sec\n",
      "    Avg latency: 675 usec (standard deviation 117 usec)\n",
      "    p50 latency: 651 usec\n",
      "    p90 latency: 780 usec\n",
      "    p95 latency: 814 usec\n",
      "    p99 latency: 907 usec\n",
      "    Avg HTTP time: 671 usec (send/recv 42 usec + response wait 629 usec)\n",
      "  Server: \n",
      "    Inference count: 8796\n",
      "    Execution count: 8796\n",
      "    Successful request count: 8796\n",
      "    Avg request latency: 445 usec (overhead 1 usec + queue 123 usec + compute input 0 usec + compute infer 321 usec + compute output 0 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 1464.6 infer/sec, latency 675 usec\n"
     ]
    }
   ],
   "source": [
    "!perf_analyzer -m dlrm -u localhost:8000 --input-data 1.json --shape CATCOLUMN:26 --shape DES:13 --shape ROWINDEX:27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e91fa",
   "metadata": {},
   "source": [
    "## 6. Get Inference result from Triton server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7bbcb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /dlrm_infer/dlrm2predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /dlrm_infer/dlrm2predict.py\n",
    "from tritonclient.utils import *\n",
    "import tritonclient.http  as httpclient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "model_name = 'dlrm'\n",
    "CATEGORICAL_COLUMNS=[\"C\" + str(x) for x in range(1, 27)]\n",
    "CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "LABEL_COLUMNS = ['label']\n",
    "emb_size_array = [4976199, 25419, 14705, 7112, 19283, 4, 6391, 1282, 60, 3289052, 282487, 138210, 11, 2203, 8901, 67, 4, 948, 15, 5577159, 1385790, 4348882, 178673, 10023, 88, 34]\n",
    "shift = np.insert(np.cumsum(emb_size_array), 0, 0)[:-1]\n",
    "test_df=pd.read_csv(\"/dlrm_infer/infer_test.csv\",sep=',')\n",
    "\n",
    "\n",
    "\n",
    "with httpclient.InferenceServerClient(\"localhost:8000\") as client:\n",
    "    dense_features = np.array([list(test_df.head(10)[CONTINUOUS_COLUMNS].values.flatten())],dtype='float32')\n",
    "    embedding_columns = np.array([list((test_df.head(10)[CATEGORICAL_COLUMNS]+shift).values.flatten())],dtype='int64')\n",
    "    row_ptrs = np.array([list(range(0,261))],dtype='int32')\n",
    "    \n",
    "    inputs = [\n",
    "        httpclient.InferInput(\"DES\", dense_features.shape,\n",
    "                              np_to_triton_dtype(dense_features.dtype)),\n",
    "        httpclient.InferInput(\"CATCOLUMN\", embedding_columns.shape,\n",
    "                              np_to_triton_dtype(embedding_columns.dtype)),\n",
    "        httpclient.InferInput(\"ROWINDEX\", row_ptrs.shape,\n",
    "                              np_to_triton_dtype(row_ptrs.dtype)),\n",
    "\n",
    "    ]\n",
    "\n",
    "    inputs[0].set_data_from_numpy(dense_features)\n",
    "    inputs[1].set_data_from_numpy(embedding_columns)\n",
    "    inputs[2].set_data_from_numpy(row_ptrs)\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"OUTPUT0\")\n",
    "    ]\n",
    "\n",
    "    response = client.infer(model_name,\n",
    "                            inputs,\n",
    "                            request_id=str(1),\n",
    "                            outputs=outputs)\n",
    "\n",
    "    result = response.get_response()\n",
    "    print(result)\n",
    "    print(\"Prediction Result:\")\n",
    "    print(response.as_numpy(\"OUTPUT0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "868a25b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1', 'model_name': 'dlrm', 'model_version': '1', 'parameters': {'NumSample': 10, 'DeviceID': 2}, 'outputs': [{'name': 'OUTPUT0', 'datatype': 'FP32', 'shape': [10], 'parameters': {'binary_data_size': 40}}]}\r\n",
      "Prediction Result:\r\n",
      "[0.02984182 0.03024833 0.03550119 0.03566186 0.04245038 0.03023028\r\n",
      " 0.02834382 0.03364136 0.02965043 0.03000181]\r\n"
     ]
    }
   ],
   "source": [
    "!python dlrm2predict.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
