{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ca5b0e9-8a5a-406d-8352-34a8b9c222a9",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Hierarchical Parameter Server (HPS) is a distributed recommendation inference framework, which combines a high-performance GPU embedding cache with an hierarchical storage architecture, to realize low-latency retrieval of embeddings for inference tasks. It is provided as a Python toolkit and can be easily integrated into the TensorFlow (TF) model graph.\n",
    "\n",
    "This tutorial will show you how to integrate HPS backend and Tensorflow backend via Triton ensemble mode. By leveraging HPS, trained Tensorflow DNN models with large embedding tables can be efficiently deployed through the Triton Inference Server. For more details about HPS, please refer to [HugeCTR Hierarchical Parameter Server (HPS)](https://nvidia-merlin.github.io/HugeCTR/master/hugectr_parameter_server.html#hugectr-hierarchical-parameter-server-database-backend).\n",
    "\n",
    "The **01_model_training.ipynb** will cover following tasks\n",
    "  * Generate mock datasets that meet the HPS input format\n",
    "  * Train native Tensorflow DNN model\n",
    "  * Separate the trained DNN model graph into two, embedding lookup and dense model graph\n",
    "  * Reconstruct the dense model graph\n",
    "  * Construct HPS lookup model, get DNN model weights and transfer to HPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3cbaaf-d9f9-40ed-83b0-44b8e56bed4a",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "ffdcbc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "e51ced60-fd3d-402e-bc96-fb7c7259a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict()\n",
    "# define model training settings\n",
    "args[\"gpu_num\"] = 4                               # the number of available GPUs\n",
    "args[\"iter_num\"] = 10                             # the number of training iteration\n",
    "args[\"embed_vec_size\"] = 16                       # the dimension of embedding vectors\n",
    "args[\"global_batch_size\"] = 65536                 # the globally batchsize for all GPUs\n",
    "args[\"slot_num\"] = 3                              # the number of feature fields in this embedding layer\n",
    "args[\"max_vocabulary_size\"] = 30000\n",
    "args[\"vocabulary_range_per_slot\"] = [[0,10000],[10000,20000],[20000,30000]]\n",
    "# define model save path\n",
    "args[\"dense_model_path\"]          = \"naive_dnn_dense.model\"\n",
    "args[\"reshape_dense_model_path\"]  = \"naive_dnn_reshape_dense.model\"\n",
    "args[\"embedding_table_path\"]      = \"naive_dnn_sparse.model\"\n",
    "# define data type\n",
    "args[\"np_key_type\"]    = np.int64\n",
    "args[\"np_vector_type\"] = np.float32\n",
    "args[\"tf_key_type\"]    = tf.int64\n",
    "args[\"tf_vector_type\"] = tf.float32\n",
    "\n",
    "# GPU environment configuration for model training\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, range(args[\"gpu_num\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46808665-c08d-4c91-9adf-f16105058918",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "Generate mock datasets that meet the HPS input format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "55c20ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_samples(num_samples, vocabulary_range_per_slot, key_dtype = args[\"np_key_type\"]):\n",
    "    \"\"\"\n",
    "    Data generator\n",
    "    \n",
    "    Returns a randomly generated set of values for keys and labels\n",
    "    \"\"\"\n",
    "    keys = list()\n",
    "    for vocab_range in vocabulary_range_per_slot:\n",
    "        keys_per_slot = np.random.randint(low=vocab_range[0], \n",
    "                                          high=vocab_range[1], \n",
    "                                          size=(num_samples, 1), \n",
    "                                          dtype=key_dtype)\n",
    "        keys.append(keys_per_slot)\n",
    "    keys = np.concatenate(np.array(keys), axis = 1)\n",
    "    labels = np.random.randint(low=0, high=2, size=(num_samples, 1))\n",
    "    return keys, labels\n",
    "\n",
    "def tf_dataset(keys, labels, batchsize):\n",
    "    \"\"\"\n",
    "    Slice tensor into batches\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((keys, labels))\n",
    "    dataset = dataset.batch(batchsize, drop_remainder=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b153bf-22ed-48df-baea-9915cac589f5",
   "metadata": {},
   "source": [
    "## Model construction, native TF DNN model for training\n",
    "We define the model graph for training with native TF layers, i.e., `tf.nn.embedding_lookup` and `tf.keras.layers.Dense`. Besides, the embedding weights are stored in `tf.Variable`. We can then train the model and extract the trained weights of the embedding table. As for the dense layers, they are saved as a separate model graph, which can be loaded directly during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c64f6260",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseModel(tf.keras.models.Model):\n",
    "    \"\"\"\n",
    "    Model with 1 input, 1 output and 3 fully-connected layer\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 init_tensors,\n",
    "                 slot_num,\n",
    "                 embed_vec_size,\n",
    "                 **kwargs):\n",
    "        super(DenseModel, self).__init__(**kwargs)\n",
    "        self.slot_num       = slot_num\n",
    "        self.embed_vec_size = embed_vec_size\n",
    "        self.init_tensors   = init_tensors\n",
    "        self.params         = tf.Variable(initial_value=tf.concat(self.init_tensors, axis=0))\n",
    "        \n",
    "        # define FC layers\n",
    "        self.fc_1 = tf.keras.layers.Dense(units=256, activation=None,\n",
    "                                                 kernel_initializer=\"ones\",\n",
    "                                                 bias_initializer=\"zeros\",\n",
    "                                                 name='fc_1')\n",
    "        self.fc_2 = tf.keras.layers.Dense(units=128, activation=None,\n",
    "                                                 kernel_initializer=\"ones\",\n",
    "                                                 bias_initializer=\"zeros\",\n",
    "                                                 name='fc_2')\n",
    "        self.fc_3 = tf.keras.layers.Dense(units=1, activation=None,\n",
    "                                                 kernel_initializer=\"ones\",\n",
    "                                                 bias_initializer=\"zeros\",\n",
    "                                                 name='fc_3')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embedding_vector = tf.nn.embedding_lookup(params=self.params, \n",
    "                                                  ids=inputs)\n",
    "        embedding_vector = tf.reshape(embedding_vector, \n",
    "                                      shape=[-1, self.slot_num * self.embed_vec_size])\n",
    "        fc1   = self.fc_1(embedding_vector)\n",
    "        fc2   = self.fc_2(fc1)\n",
    "        logit = self.fc_3(fc2)\n",
    "        return logit, embedding_vector\n",
    "\n",
    "    def summary(self):\n",
    "        inputs = tf.keras.Input(shape=(self.slot_num,), \n",
    "                                dtype=args[\"tf_key_type\"], \n",
    "                                name=\"input_dense\")\n",
    "        model  = tf.keras.models.Model(inputs=inputs, \n",
    "                                       outputs=self.call(inputs),\n",
    "                                       name='tf_model')\n",
    "        return model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "0c7e7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    \"\"\"\n",
    "    Define TF DNN model training process\n",
    "    \"\"\"\n",
    "    # initialize input tensor\n",
    "    init_tensors = np.ones(shape=[args[\"max_vocabulary_size\"], args[\"embed_vec_size\"]], \n",
    "                           dtype=args[\"np_vector_type\"])\n",
    "    \n",
    "    # model construction\n",
    "    model = DenseModel(init_tensors, args[\"slot_num\"], args[\"embed_vec_size\"])\n",
    "    model.summary()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    \n",
    "    # define model training steps\n",
    "    def _train_step(inputs, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logit, embedding_vector = model(inputs)\n",
    "            loss = loss_fn(labels, logit)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        return logit, embedding_vector, loss\n",
    "\n",
    "    # prepare dataset\n",
    "    keys, labels = generate_random_samples(args[\"global_batch_size\"] * args[\"iter_num\"], \n",
    "                                           args[\"vocabulary_range_per_slot\"],  \n",
    "                                           args[\"np_key_type\"])\n",
    "    dataset = tf_dataset(keys, labels, args[\"global_batch_size\"])\n",
    "    \n",
    "    # model training\n",
    "    for i, (id_tensors, labels) in enumerate(dataset):\n",
    "        _, embedding_vector, loss = _train_step(id_tensors, labels)\n",
    "        print(\"-\"*20, \"Step {}, loss: {}\".format(i, loss),  \"-\"*20)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b651759a-bcf9-4a00-a580-936db6625df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.compat.v1.nn.embedding_lookup_9), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(30000, 16) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "Model: \"tf_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_dense (InputLayer)    [(None, 3)]               0         \n",
      "                                                                 \n",
      " tf.compat.v1.nn.embedding_l  (None, 3, 16)            0         \n",
      " ookup_9 (TFOpLambda)                                            \n",
      "                                                                 \n",
      " tf.reshape_9 (TFOpLambda)   (None, 48)                0         \n",
      "                                                                 \n",
      " fc_1 (Dense)                (None, 256)               12544     \n",
      "                                                                 \n",
      " fc_2 (Dense)                (None, 128)               32896     \n",
      "                                                                 \n",
      " fc_3 (Dense)                (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45,569\n",
      "Trainable params: 45,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "-------------------- Step 0, loss: 785424.0 --------------------\n",
      "-------------------- Step 1, loss: 516179.40625 --------------------\n",
      "-------------------- Step 2, loss: 325251.8125 --------------------\n",
      "-------------------- Step 3, loss: 198967.75 --------------------\n",
      "-------------------- Step 4, loss: 115826.421875 --------------------\n",
      "-------------------- Step 5, loss: 64801.0859375 --------------------\n",
      "-------------------- Step 6, loss: 34508.4140625 --------------------\n",
      "-------------------- Step 7, loss: 17323.46875 --------------------\n",
      "-------------------- Step 8, loss: 8158.732421875 --------------------\n",
      "-------------------- Step 9, loss: 3415.898193359375 --------------------\n"
     ]
    }
   ],
   "source": [
    "trained_model = train(args)\n",
    "\n",
    "# get trained model weights for HPS lookup\n",
    "weights_list  = trained_model.get_weights()\n",
    "embedding_weights = weights_list[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9f0026-5a4b-4905-9170-098d03301bb3",
   "metadata": {},
   "source": [
    "## Save dense model graph\n",
    "Separate the trained DNN model graph into two, embedding layer and dense model graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "7c99a56c-33e9-4ba8-aff6-57906a21ee91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_dense_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 48)]              0         \n",
      "                                                                 \n",
      " fc_1 (Dense)                (None, 256)               12544     \n",
      "                                                                 \n",
      " fc_2 (Dense)                (None, 128)               32896     \n",
      "                                                                 \n",
      " fc_3 (Dense)                (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45,569\n",
      "Trainable params: 45,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: naive_dnn_dense.model/assets\n"
     ]
    }
   ],
   "source": [
    "# save dense layers as a seperate model graph\n",
    "dense_model = tf.keras.models.Model(trained_model.get_layer(\"fc_1\").input, \n",
    "                                    trained_model.get_layer(\"fc_3\").output, \n",
    "                                    name='tf_dense_model')\n",
    "dense_model.summary()\n",
    "\n",
    "# saved dense model graph will be load directly in the Triton inference part\n",
    "dense_model.save(args[\"dense_model_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0975b3-07a2-4786-8942-f3c1061d82d2",
   "metadata": {},
   "source": [
    "## Reconstruct the dense model graph\n",
    "Add reshape layer to top of loaded dense model to meet input format of Triton Tensorflow backend. \n",
    "\n",
    "For a Triton input, a model that supports batching expects a batched input to have shape [batch-size], which means that the batch dimension fully describes the shape. For the inference API the equivalent shape [batch-size, 1] must be specified since each input must specify a non-empty dims [[link](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md#reshape)]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "98d53b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeDenseModel(tf.keras.models.Model):\n",
    "    \"\"\"\n",
    "    Add a reshape layer on top of the loaded dense model,\n",
    "    to support Triton's tensorflow backend input format.\n",
    "    The output of HPS is 1-dimension [-1], while TF backend requires 2-dimensional input [batch_size, -1].\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 slot_num,\n",
    "                 embed_vec_size,\n",
    "                 dense_model_path,\n",
    "                 **kwargs):\n",
    "        super(ReshapeDenseModel, self).__init__(**kwargs)\n",
    "        self.slot_num = slot_num\n",
    "        self.embed_vec_size = embed_vec_size\n",
    "        self.dense_model = tf.keras.models.load_model(dense_model_path)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        reshaped_input_vector = tf.reshape(inputs, \n",
    "                                           shape=[-1, self.slot_num * self.embed_vec_size])\n",
    "        logit = self.dense_model(reshaped_input_vector)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "e3a299bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "reshape_dense_model = ReshapeDenseModel(args[\"slot_num\"], \n",
    "                                        args[\"embed_vec_size\"], \n",
    "                                        args[\"dense_model_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "4f8a29d5-40cb-4ed9-8ca2-7018198702f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For reshape dense model, input shape is:(96,) output shape is:(2, 1)\n"
     ]
    }
   ],
   "source": [
    "# test the new reshaped dense model\n",
    "input_test  = np.random.random((96)).astype(np.float32)\n",
    "output_test = reshape_dense_model(input_test)\n",
    "print(\"For reshape dense model, input shape is:{}\".format(input_test.shape), \n",
    "      \"output shape is:{}\".format(output_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "d7d6758f-8b44-4e2e-a564-78bf9b1cbc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: naive_dnn_reshape_dense.model/assets\n"
     ]
    }
   ],
   "source": [
    "# save reshaped dense model graph, \n",
    "reshape_dense_model.save(args[\"reshape_dense_model_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3808c5-9521-441f-ac7b-9c70365d16f2",
   "metadata": {},
   "source": [
    "## Construct HPS lookup model\n",
    "In order to leverage HPS to facilitate the embedding lookup part, we need to convert the model lookup part of the model into a [format supported by HPS](../../docs/architecture.md#hierarchical-parameter-server-input-format). \n",
    "In this case, `tf.nn.embedding_lookup` of native TF DNN model will be replaced by HPS. Embedding weights of dense models are reloaded and transferred to HPS (sparse models) in preparation for future inference deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "51b189ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sparse_model(embeddings_weights, embedding_table_path, embedding_vec_size):\n",
    "    \"\"\"\n",
    "    Convert the lookup part of the model to a format supported by HPS (key-vector pair files),\n",
    "    the embedding weights of the trained dense model will be reloaded.\n",
    "    \n",
    "    Outputs(key-vector pair files) will be saved to defined sparse model path\n",
    "    \"\"\"\n",
    "    os.system(\"mkdir -p {}\".format(embedding_table_path))\n",
    "    \n",
    "    with open(\"{}/key\".format(embedding_table_path), 'wb') as key_file, \\\n",
    "        open(\"{}/emb_vector\".format(embedding_table_path), 'wb') as vec_file:\n",
    "        for key in range(embeddings_weights.shape[0]):\n",
    "            vec = embeddings_weights[key]\n",
    "            key_struct = struct.pack('q', key)\n",
    "            vec_struct = struct.pack(str(embedding_vec_size) + \"f\", *vec)\n",
    "            key_file.write(key_struct)\n",
    "            vec_file.write(vec_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "540ec43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_sparse_model(embedding_weights, \n",
    "                        args[\"embedding_table_path\"], \n",
    "                        args[\"embed_vec_size\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
